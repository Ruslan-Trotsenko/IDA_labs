{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import ops\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n"
      ],
      "metadata": {
        "id": "4SzBzooW1Afu"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "import gdown\n",
        "\n",
        "file_id = \"1AsV1_sa3T1EQY2Nf2r-ISFOJLScjcZ_V\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "gdown.download(url, \"ukr.txt\", quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "MharCvRxVeLA",
        "outputId": "e47c0dbb-c23e-4863-af76-d00ee2e2e31c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AsV1_sa3T1EQY2Nf2r-ISFOJLScjcZ_V\n",
            "To: /content/ukr.txt\n",
            "100%|██████████| 25.0M/25.0M [00:00<00:00, 64.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ukr.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ukr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ukr, _ = line.split(\"\\t\")\n",
        "    ukr = \"[start] \" + ukr + \" [end]\"\n",
        "    text_pairs.append((eng, ukr))\n",
        "\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "\n",
        "text_pairs = text_pairs[:50000]\n"
      ],
      "metadata": {
        "id": "KaVQzyCSVhac"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_val = int(0.15 * len(text_pairs))\n",
        "train_pairs = text_pairs[:-num_val]\n",
        "val_pairs = text_pairs[-num_val:]\n"
      ],
      "metadata": {
        "id": "zcYKElYOVqqf"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 10000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "embed_dim = 128\n",
        "latent_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "epochs = 5\n"
      ],
      "metadata": {
        "id": "7fE_04UnVuOQ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation.replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "\n",
        "def ukr_standardization(input_string):\n",
        "    return tf.strings.regex_replace(\n",
        "        tf.strings.lower(input_string),\n",
        "        \"[%s]\" % re.escape(strip_chars),\n",
        "        \"\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "YMxYyaoZVxKJ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vector = layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "ukr_vector = layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=ukr_standardization,\n",
        ")\n",
        "\n",
        "train_eng = [pair[0] for pair in train_pairs]\n",
        "train_ukr = [pair[1] for pair in train_pairs]\n",
        "\n",
        "eng_vector.adapt(train_eng)\n",
        "ukr_vector.adapt(train_ukr)\n"
      ],
      "metadata": {
        "id": "ReJ7R_S2Vz-r"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, ukr):\n",
        "    eng = eng_vector(eng)\n",
        "    ukr = ukr_vector(ukr)\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": ukr[:, :-1],\n",
        "        },\n",
        "        ukr[:, 1:]\n",
        "    )\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ukr_texts = zip(*pairs)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(ukr_texts)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.cache().shuffle(2048).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n"
      ],
      "metadata": {
        "id": "JTkm5V8MV0wd"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocabulary_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocabulary_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(0, length, 1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return ops.not_equal(inputs, 0)\n"
      ],
      "metadata": {
        "id": "wMeD2kMWV2uB"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = ops.cast(mask[:, None, :], \"int32\")\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ],
      "metadata": {
        "id": "Hmuif81dV47Z"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(latent_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
        "        i = ops.arange(seq_len)[:, None]\n",
        "        j = ops.arange(seq_len)\n",
        "        mask = ops.cast(i >= j, \"int32\")\n",
        "        mask = ops.reshape(mask, (1, seq_len, seq_len))\n",
        "        return ops.tile(mask, [batch_size, 1, 1])\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        inputs, encoder_outputs = inputs\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            inputs, inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            out_1, encoder_outputs\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n"
      ],
      "metadata": {
        "id": "HKbt4xMzV7gB"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocabulary_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocabulary_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)([x, encoder_outputs])\n",
        "x = layers.Dropout(0.3)(x)\n",
        "decoder_outputs = layers.Dense(vocabulary_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model(\n",
        "    {\"encoder_inputs\": encoder_inputs, \"decoder_inputs\": decoder_inputs},\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "ENXF5GeGV6rj"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "transformer.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "tMc5ZEZdWAu6",
        "outputId": "6ff4a738-7aba-4fa4-cf44-d141bfec403a"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,282,560\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_12        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,282,560\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m659,712\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │ not_equal_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m1,187,456\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,290,000\u001b[0m │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│                     │ \u001b[38;5;34m10000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,282,560</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_12        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,282,560</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">659,712</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ not_equal_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,187,456</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,702,288\u001b[0m (21.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,702,288</span> (21.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,702,288\u001b[0m (21.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,702,288</span> (21.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1jwFFklWDSL",
        "outputId": "5333d684-2f4f-4912-d5fa-c6ca9bd5de67"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1251s\u001b[0m 2s/step - accuracy: 0.1476 - loss: 4.7072 - val_accuracy: 0.2018 - val_loss: 2.5454\n",
            "Epoch 2/5\n",
            "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1255s\u001b[0m 2s/step - accuracy: 0.2050 - loss: 2.4612 - val_accuracy: 0.2205 - val_loss: 1.9446\n",
            "Epoch 3/5\n",
            "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1232s\u001b[0m 2s/step - accuracy: 0.2307 - loss: 1.7033 - val_accuracy: 0.2314 - val_loss: 1.6197\n",
            "Epoch 4/5\n",
            "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1228s\u001b[0m 2s/step - accuracy: 0.2465 - loss: 1.2831 - val_accuracy: 0.2370 - val_loss: 1.5014\n",
            "Epoch 5/5\n",
            "\u001b[1m665/665\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1232s\u001b[0m 2s/step - accuracy: 0.2586 - loss: 1.0198 - val_accuracy: 0.2388 - val_loss: 1.4645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b71c46630b0>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Словник індекс → слово\n",
        "ukr_vocab = ukr_vector.get_vocabulary()\n",
        "ukr_index_lookup = dict(enumerate(ukr_vocab))\n",
        "\n",
        "max_decoded_sentence_length = sequence_length\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    # 1️⃣ Векторизуємо англійське речення\n",
        "    tokenized_input_sentence = eng_vector([input_sentence])\n",
        "\n",
        "    # 2️⃣ Стартовий токен\n",
        "    decoded_sentence = \"[start]\"\n",
        "\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # 3️⃣ Векторизуємо поточний вихід декодера\n",
        "        tokenized_target_sentence = ukr_vector([decoded_sentence])[:, :-1]\n",
        "\n",
        "        # 4️⃣ Прогноз\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": tokenized_input_sentence,\n",
        "                \"decoder_inputs\": tokenized_target_sentence,\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        # 5️⃣ Беремо токен з максимальною ймовірністю\n",
        "        sampled_token_index = int(np.argmax(predictions[0, i]))\n",
        "        sampled_token = ukr_index_lookup[sampled_token_index]\n",
        "\n",
        "        # 6️⃣ Додаємо слово до результату\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        # 7️⃣ Умова завершення\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "SB-FShjGoH79"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in val_pairs]\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "\n",
        "    print(f\"<I>: {input_sentence}\")\n",
        "    print(f\"<O>: {translated}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDelJLfpoLu-",
        "outputId": "085900c9-bac8-4147-c017-9b95648d4a7d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<I>: I haven't paid yet.\n",
            "<O>: [start] Я ще не заплатив [end]\n",
            "\n",
            "<I>: I thought Tom would be interested in this.\n",
            "<O>: [start] Я думав що Том це цікаво [end]\n",
            "\n",
            "<I>: You must be Tom.\n",
            "<O>: [start] Ти мабуть Том [end]\n",
            "\n",
            "<I>: They have gone to Europe.\n",
            "<O>: [start] Вони пішли в Європі [end]\n",
            "\n",
            "<I>: He lost his life in an accident.\n",
            "<O>: [start] Він втратив життя у аварії [end]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}